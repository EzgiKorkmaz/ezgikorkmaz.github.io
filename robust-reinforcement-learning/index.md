---
layout: default
title: Robust Reinforcement Learning 
keywords: "Deep Reinforcement Learning, Adversarial Attacks, Robustness, AI Safety, black-box attack, generalization"
---

<style>
  /* This targets the specific 'Minimal' theme layout */
  .wrapper {
    max-width: 900px !important; /* Makes the page wider */
    margin-left: auto !important;
    margin-right: auto !important;
  }
  header {
    display: none !important; /* This usually removes the sidebar/picture */
  }
  section {
    width: 100% !important;
    float: none !important;
  }
</style>


<div style="text-align: center; margin-bottom: 40px;">
  


  <h3 style="color: #003366; margin-top: 10px;">
    Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness
  </h3>

  <p style="color: #003366; font-weight: bold; font-size: 1.1em; margin-top: 5px;">
    AAAI Conference on Artificial Intelligence [Acceptance Rate: 19.6%], AAAI 2023.
  </p>

</div>

Learning from raw high dimensional data via interaction with a given environment has been effectively achieved through the utilization of deep neural networks. Yet the observed degradation in policy performance caused by imperceptible worst-case policy dependent translations along high sensitivity directions (i.e. adversarial perturbations) raises concerns on the robustness of deep reinforcement learning policies. In our paper, we show that these high sensitivity directions do not lie only along particular worst-case directions, but rather are more abundant in the deep neural policy landscape and can be found via more natural means in a black-box setting. Furthermore, we show that vanilla training techniques intriguingly result in learning more robust policies compared to the policies learnt via the state-of-the-art adversarial training techniques. We believe our work lays out intriguing properties of the deep reinforcement learning policy manifold and our results can help to build robust and generalizable deep reinforcement learning policies.


<div style="text-align: center; margin: 30px 0;">
  
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26009/25781" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      Paper
    </button>
  </a>

  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26009" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      AAAI
    </button>
  </a>

<a href="https://blogs.ucl.ac.uk/steapp/tag/adversarial-reinforcement-learning/" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      Blog
    </button>
  </a>

  <a href="https://underline.io/lecture/69381-adversarial-robust-deep-reinforcement-learning-requires-redefining-robustness" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      Video
    </button>
  </a>

  <a href="https://ezgikorkmaz.github.io" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      Author
    </button>
  </a>

</div>


#### Paper Highlights

✨Robust reinforcement learning is vulnerable to black-box adversarial attacks.

✨Standard reinforcement learning can generalize much better than robust reinforcement learning. 

✨Standard reinforcement learning is robust to imperceptible natural perturbations.

✨ Robust reinforcement learning has extreme sensitivity to imperceptible natural perturbations.




#### Citation

<div style="display: flex; justify-content: center; margin-top: 20px;">
  <div style="background-color: #fcfcfc; padding: 20px; border-radius: 8px; border: 1px solid #d1d5da; font-family: 'Courier New', Courier, monospace; line-height: 1.5; width: 100%; max-width: 850px;">
<pre style="margin: 0; white-space: pre-wrap; color: #000000 !important;"><span style="color: #000000; font-weight: bold;">@inproceedings</span>{<span style="color: #003366;">Korkmazaaai23</span>,
  <span style="color: #000000; font-weight: bold;">author</span>={<span style="color: #003366;">Korkmaz, Ezgi</span>},
  <span style="color: #000000; font-weight: bold;">title</span>={<span style="color: #003366;">Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness</span>},
  <span style="color: #000000; font-weight: bold;">booktitle</span>={<span style="color: #003366;">Thirty-Seventh AAAI Conference on Artificial Intelligence</span>},
  <span style="color: #000000; font-weight: bold;">pages</span>={<span style="color: #003366;">8369--8377</span>},
  <span style="color: #000000; font-weight: bold;">year</span>={<span style="color: #003366;">2023</span>},
  <span style="color: #000000; font-weight: bold;">url</span>={<a href="https://doi.org/10.1609/aaai.v37i7.26009" style="color: #003366; text-decoration: underline;">https://doi.org/10.1609/aaai.v37i7.26009</a>}
}</pre>
  </div>
</div>
