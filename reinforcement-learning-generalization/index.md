---
layout: default
title: Reinforcement Learning Generalization
keywords: "Deep Reinforcement Learning, generalization, DeepRL, reinforcement learning, generalisation"
---

<style>
  /* This targets the specific 'Minimal' theme layout */
  .wrapper {
    max-width: 900px !important; /* Makes the page wider */
    margin-left: auto !important;
    margin-right: auto !important;
  }
  header {
    display: none !important; /* This usually removes the sidebar/picture */
  }
  section {
    width: 100% !important;
    float: none !important;
  }
</style>


<div style="text-align: center; margin-bottom: 40px;">



  <h3 style="color: #003366; margin-top: 10px;">
    A Survey Analyzing Generalization in Deep Reinforcement Learning
  </h3>

</div>

Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.


<div style="text-align: center; margin: 30px 0;">
  
  <a href="https://arxiv.org/pdf/2401.02349v2" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      Paper
    </button>
  </a>

  <a href="https://ezgikorkmaz.github.io" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      Author
    </button>
  </a>

</div>


### Citation

<div style="display: flex; justify-content: center; margin-top: 20px;">
  <div style="background-color: #fcfcfc; padding: 20px; border-radius: 8px; border: 1px solid #d1d5da; font-family: 'Courier New', Courier, monospace; line-height: 1.5; width: 100%; max-width: 850px;">
<pre style="margin: 0; white-space: pre-wrap; color: #000000 !important;"><span style="color: #000000; font-weight: bold;">@article</span>{<span style="color: #003366;">rlsurveykorkmaz</span>,
  <span style="color: #000000; font-weight: bold;">title</span>={<span style="color: #003366;">A Survey Analyzing Generalization in Deep Reinforcement Learning</span>},
  <span style="color: #000000; font-weight: bold;">author</span>={<span style="color: #003366;">Korkmaz, Ezgi</span>},
   <span style="color: #000000; font-weight: bold;">journal</span>={<span style="color: #003366;">ArXiv</span>},
  <span style="color: #000000; font-weight: bold;">year</span>={<span style="color: #003366;">2024</span>},
  <span style="color: #000000; font-weight: bold;">url</span>={<a href="https://doi.org/10.48550/arXiv.2401.02349" style="color: #003366; text-decoration: underline;">https://doi.org/10.48550/arXiv.2401.02349</a>}
}</pre>
  </div>
</div>
