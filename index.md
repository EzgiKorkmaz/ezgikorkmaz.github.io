<head>
  <meta name="keywords" content="adversarial, deep reinforcement learning, machine learning, adversarial attacks, robust, DeepRL, DRL, adversarial policies, robust reinforcement learning, safe RL, AI safety, AI security, machine learning safety, adversarial machine learning, reinforcement learning, deep learning, explainability, interpretability, AI alignment, ML safety, ML security, machine learning safety, artificial intelligence safety, adversarial reinforcement learning, robustness, robust RL, adversarial RL, safe reinforcement learning, safe RL, RL security, reinforcement learning security, AI security, adversarial machine learning, human centered AI ">
</head>


I am a machine learning researcher and a PhD candidate in artificial intelligence and machine learning. I hold a Masters of Science degree in Electrical Engineering and Information Technology from Karlsruhe Institute of Technology. I wrote my MSc thesis at University of California, Berkeley.  Recently, I have been at DeepMind. My research focus is robustness and generalization in machine learning. 



### Single Author Publications

[1] Ezgi Korkmaz. Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness. AAAI Conference on Artificial Intelligence, 2023. **[Acceptance Rate: 19.6%]** <br /> [[AAAI]](https://ojs.aaai.org/index.php/AAAI/article/view/26009) [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/26009/25781) [[Cite]](ezgikorkmazaaai23.html)

[2<sup>a</sup>] Ezgi Korkmaz et al. Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions. International Conference on Machine Learning, ICML 2023. **[Acceptance Rate: 27.94%]** <br /> 
[[Paper]](https://openreview.net/forum?id=JS2iSqVZlN) [[Cite]](ezgikorkmazicml23.html)

[3] Ezgi Korkmaz. Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs. AAAI Conference on Artificial Intelligence, 2022. **[Acceptance Rate: 14.58%]** <br />
[[AAAI]](https://aaai.org/papers/07229-deep-reinforcement-learning-policies-learn-shared-adversarial-features-across-mdps/) [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/20684/20443) [[Abstract]](https://adversarialreinforcementlearning.github.io) [[BibTeX]](https://dblp.org/rec/conf/aaai/Korkmaz22.html?view=bibtex) [[Cite]](ekaaai22.html) <br />
[[MILA Blog]](https://mila.quebec/en/article/adversarial-deep-reinforcement-learning/) [[In French]](https://mila.quebec/article/apprentissage-par-renforcement-profond-de-maniere-antagoniste/) [[Twitter]](https://twitter.com/Mila_Quebec/status/1636472805620428809?cxt=HHwWksC9-ZTW9bUtAAAA) 

[4] Ezgi Korkmaz. Investigating Vulnerabilities of Deep Neural Policies. Conference on Uncertainty in Artificial Intelligence (UAI), Proceedings of Machine Learning Research (PMLR), 2021.<br />
**[Acceptance Rate: 26.38%]** <br />
[[PMLR]](https://proceedings.mlr.press/v161/korkmaz21a.html) [[Paper]](https://proceedings.mlr.press/v161/korkmaz21a/korkmaz21a.pdf) [[Abstract]](https://robustdeepreinforcementlearning.github.io/) [[News]](https://adversa.ai/blog/best-of-adversarial-ml-week-34-attacking-aerial-imagery-object-detector/) [[Cite]](ekuaibibtex.html)

[5] Disclosing the Biases in Large Language Models via Reward Structured Questions. Conference on Neural Information Processing Systems (NeurIPS) Workshop on Interactive Learning for Natural Language Processing, 2022 & Conference on Neural Information Processing Systems (NeurIPS) Foundation Models for Decision Making Workshop, 2022 & Conference on Neural Information Processing Systems (NeurIPS) Robustness in Sequence Modeling Workshop, 2022 & Conference on Neural Information Processing Systems (NeurIPS) Machine Learning Safety Workshop, 2022. [[Cite]](neurips2022.html)

[6] Ezgi Korkmaz. Spectral Robustness Analysis of Deep Imitation Learning. Conference on Neural Information Processing Systems (NeurIPS) Machine Learning Safety Workshop, 2022.

[7] Ezgi Korkmaz. Inaccuracy of State-Action Value Function For Non-Optimal Actions in Adversarially Trained Deep Neural Policies. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPR) **[Oral Presentation]**, 2021 & International Conference on Learning Representation (ICLR) Robust and Reliable Machine Learning in the Real World Workshop, 2021.[[Paper]](https://ieeexplore.ieee.org/document/9523170) [[Slides]](https://www.youtube.com/watch?v=F3cvXrLWcoU&t=3s&ab_channel=AngelinaWang) [[Cite]](https://dblp.org/rec/conf/cvpr/Korkmaz21.html?view=bibtex)

[8] Ezgi Korkmaz. Robustness of Inverse Reinforcement Learning. International Conference on Machine Learning (ICML) Artificial Intelligence for Agent Based Modelling Workshop, 2022 & Conference on Neural Information Processing Systems (NeurIPS) Machine Learning Safety Workshop, 2022. [[Cite]](ekicml22bibtex.html)

[9] Ezgi Korkmaz. Adversarial Attacks Against Deep Imitation and Inverse Reinforcement Learning. International Conference on Machine Learning (ICML) Complex Feedback in Online Learning Workshop, 2022. [[Cite]](ekicmlbibtex.html)

[10] Ezgi Korkmaz. A Brief Summary on COVID-19 Pandemic and Machine Learning Approaches. International Joint Conference on Artificial Intelligence (IJCAI) Workshop on Artificial Intelligence for Social Good, 2021 & Conference on Neural Information Processing Systems (NeurIPS) Machine Learning in Public Health Workshop.<br />
**[Oral Presentation]**. [[Paper NeurIPS]](neurIPS21.pdf) [[Abstract]](https://machinelearningcovid19.github.io/) [[Cite]](ekijcaibibtex.html)

[11] Ezgi Korkmaz. Non-Robust Feature Mapping in Deep Reinforcement Learning. International Conference on Machine Learning (ICML) A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning Workshop, 2021 & Conference on Neural Information Processing Systems (NeurIPS) Metacognition in the Age of AI: Challenges and Opportunities **[Spotlight Presentation]**, 2021. [[Cite]](icmlmapbibtex.html)

[12]  Ezgi Korkmaz. Adversarial Training Blocks Generalization in Neural Policies. International Conference on Learning Representation (ICLR) Robust and Reliable Machine Learning in the Real World Workshop, 2021 & Conference on Neural Information Processing Systems (NeurIPS) Workshop on Distribution Shifts: Connecting Methods and Applications, 2021 & Conference on Neural Information Processing Systems (NeurIPS) Safe and Robust Control of Uncertain Systems Workshop, 2021 & Conference on Neural Information Processing Systems (NeurIPS) I Can't Believe It's Not Better Workshop, 2021. [[Paper ICLR]](iclr.pdf) [[Paper NeurIPS]](KorkmazNeurIPS.pdf) [[Cite]](eknaturalbibtex.html)

[12] Ezgi Korkmaz. Nesterov Momentum Adversarial Perturbations in the Deep Reinforcement Learning Domain. International Conference on Machine Learning (ICML) Inductive Biases, Invariances and Generalization in Reinforcement Learning Workshop, 2020. [[Paper]](https://biases-invariances-generalization.github.io/pdf/big_33.pdf) [[Cite]](ekicmlnesterovbibtex.html)

[14] Ezgi Korkmaz. Adversarially Trained Neural Policies in Fourier Domain. International Conference on Learning Representation (ICLR) Robust and Reliable Machine Learning in the Real World Workshop,2021 & International Conference on Machine Learning (ICML) A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning Workshop, 2021. [[Cite]](ekfourierbibtex.html)

<sup>a</sup> Excluding the study [2].

