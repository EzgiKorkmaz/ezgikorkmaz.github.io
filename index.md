<head>
  <meta name="keywords" content="reinforcement learning, adversarial, deep reinforcement learning, machine learning, adversarial attacks, robust, DeepRL, DRL, adversarial policies, robust reinforcement learning, safe RL, AI safety, AI security, machine learning safety, adversarial machine learning, reinforcement learning, deep learning, explainability, interpretability, AI alignment, ML safety, ML security, machine learning safety, artificial intelligence safety, adversarial reinforcement learning, robustness, robust RL, adversarial RL, safe reinforcement learning, safe RL, RL security, reinforcement learning security, AI security, adversarial machine learning, human centered AI ">
</head>


I am a machine learning researcher and I hold a PhD in artificial intelligence and machine learning from University College London (UCL). I wrote my MSc thesis at University of California, Berkeley. Recently, I have been at DeepMind. My research focus is robustness and generalization in machine learning. 



### Single Author Publications

[1] Ezgi Korkmaz. Counteractive RL: Rethinking Core Principles for Efficient and Scalable Deep Reinforcement Learning. Conference on Neural Information Processing Systems, **NeurIPS 2025**. <br /> 
**Spotlight Presentation**

[1] Ezgi Korkmaz. Understanding and Diagnosing Deep Reinforcement Learning. International Conference on Machine Learning, **ICML 2024**.  <br /> 
**[[ICML 2024]](https://openreview.net/pdf?id=s9RKqT7jVM)** 
[[Paper]](https://proceedings.mlr.press/v235/korkmaz24a.html) 
[[Cite]](ezgikorkmazicml24.html)
[[BibTeX]](https://dblp.org/rec/conf/icml/Korkmaz24.html?view=bibtex)

[2] Ezgi Korkmaz. Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness. AAAI Conference on Artificial Intelligence [Acceptance Rate: 19.6%], **AAAI 2023**. <br /> 
**[[AAAI 2023]](https://ojs.aaai.org/index.php/AAAI/article/view/26009)**
[[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/26009/25781) 
[[Cite]](ezgikorkmazaaai23.html)
[[BibTeX]](https://dblp.org/rec/conf/aaai/Korkmaz23.html?view=bibtex) 
[[In UCL Blog]](https://blogs.ucl.ac.uk/steapp/2023/11/15/adversarial-attacks-robustness-and-generalization-in-deep-reinforcement-learning/) 


[3<sup>a</sup>] Ezgi Korkmaz et al. Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions. International Conference on Machine Learning [Acceptance Rate: 27.94%], **ICML 2023**.  <br /> 
**[[ICML 2023]](https://proceedings.mlr.press/v202/korkmaz23a.html)** 
[[Paper]](https://proceedings.mlr.press/v202/korkmaz23a/korkmaz23a.pdf) 
[[Cite]](ezgikorkmazicml23.html)
[[BibTeX]](https://dblp.org/rec/conf/icml/KorkmazB23.html?view=bibtex)

[4] Ezgi Korkmaz. Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs. AAAI Conference on Artificial Intelligence  [Acceptance Rate: 14.58%],  **AAAI 2022**.<br />
**[[AAAI 2022]](https://aaai.org/papers/07229-deep-reinforcement-learning-policies-learn-shared-adversarial-features-across-mdps/)** 
[[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/20684/20443) [[Abstract]](https://adversarialreinforcementlearning.github.io) 
[[BibTeX]](https://dblp.org/rec/conf/aaai/Korkmaz22.html?view=bibtex) [[Cite]](ekaaai22.html) <br />
[[In MILA Blog]](https://mila.quebec/en/article/adversarial-deep-reinforcement-learning/) 
[[In French]](https://mila.quebec/article/apprentissage-par-renforcement-profond-de-maniere-antagoniste/) 
[[Twitter]](https://twitter.com/Mila_Quebec/status/1636472805620428809?cxt=HHwWksC9-ZTW9bUtAAAA) 

[5] Ezgi Korkmaz. Investigating Vulnerabilities of Deep Neural Policies. Conference on Uncertainty in Artificial Intelligence (UAI), Proceedings of Machine Learning Research (PMLR) [Acceptance Rate: 26.38%], **PMLR 2021**.<br />
**[[PMLR 2021]](https://proceedings.mlr.press/v161/korkmaz21a.html)** 
[[Paper]](https://proceedings.mlr.press/v161/korkmaz21a/korkmaz21a.pdf) 
[[Abstract]](https://robustdeepreinforcementlearning.github.io/) 
[[News]](https://adversa.ai/blog/best-of-adversarial-ml-week-34-attacking-aerial-imagery-object-detector/) 
[[BibTeX]](https://dblp.org/rec/conf/uai/Korkmaz21.html?view=bibtex)
[[Cite]](ekuaibibtex.html)

[6] Ezgi Korkmaz. Revealing the Bias in Large Language Models via Reward Structured Questions. Conference on Neural Information Processing Systems (NeurIPS) Workshop on Interactive Learning for Natural Language Processing, 2022 & Conference on Neural Information Processing Systems (NeurIPS) Foundation Models for Decision Making Workshop, 2022 & Conference on Neural Information Processing Systems (NeurIPS) Robustness in Sequence Modeling Workshop, 2022 & Conference on Neural Information Processing Systems (NeurIPS) Machine Learning Safety Workshop, 2022.
 [[Paper]](KorkmazNeurIPS22.pdf)
 [[Cite]](neurips2022.html)

 [7] Ezgi Korkmaz. A Survey Analyzing Generalization in Deep Reinforcement Learning. Conference on Neural Information Processing Systems (NeurIPS) Robot Learning Workshop, NeurIPS 2023. 
 [[ArXiv]](https://arxiv.org/pdf/2401.02349v2)
 [[Paper]](Reinforcement_Learning_Survey_NeurIPS23.pdf)
 [[Cite]](rlsurveyneurips2023.html)

[8] Ezgi Korkmaz. Adversarial Robust Deep Reinforcement Learning is Neither Robust Nor Safe. Conference on Neural Information Processing Systems (NeurIPS) Workshop on Statistical Foundations of LLMs and Foundation Models, NeurIPS 2024.
[[Paper]](https://openreview.net/pdf?id=EPa0udvXJE)
[[GitHub]](RobustDeepReinforcementLearningNeurIPS2024.pdf)
[[Cite]](ezgikorkmazneurips24.html)

[9] Ezgi Korkmaz. Spectral Robustness Analysis of Deep Imitation Learning. Conference on Neural Information Processing Systems (NeurIPS) Machine Learning Safety Workshop, 2022.

[10] Ezgi Korkmaz. Inaccuracy of State-Action Value Function For Non-Optimal Actions in Adversarially Trained Deep Neural Policies. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPR) **[Oral Presentation]**, 2021 & International Conference on Learning Representation (ICLR) Robust and Reliable Machine Learning in the Real World Workshop, 2021.[[Paper]](https://ieeexplore.ieee.org/document/9523170) 
[[BibTeX]](https://dblp.org/rec/conf/cvpr/Korkmaz21.html?view=bibtex)
[[Slides]](https://www.youtube.com/watch?v=F3cvXrLWcoU&t=3s&ab_channel=AngelinaWang) 
[[Cite]](https://dblp.org/rec/conf/cvpr/Korkmaz21.html?view=bibtex)

[11] Ezgi Korkmaz. Robustness of Inverse Reinforcement Learning. International Conference on Machine Learning (ICML) Artificial Intelligence for Agent Based Modelling Workshop, 2022 & Conference on Neural Information Processing Systems (NeurIPS) Machine Learning Safety Workshop, 2022. [[Cite]](ekicml22bibtex.html)

[12] Ezgi Korkmaz. Adversarial Attacks Against Deep Imitation and Inverse Reinforcement Learning. International Conference on Machine Learning (ICML) Complex Feedback in Online Learning Workshop, 2022. [[Cite]](ekicmlbibtex.html)

[12] Ezgi Korkmaz. A Brief Summary on COVID-19 Pandemic and Machine Learning Approaches. International Joint Conference on Artificial Intelligence (IJCAI) Workshop on Artificial Intelligence for Social Good, 2021 & Conference on Neural Information Processing Systems (NeurIPS) Machine Learning in Public Health Workshop.<br />
**[Oral Presentation]**. [[Paper NeurIPS]](neurIPS21.pdf) [[Abstract]](https://machinelearningcovid19.github.io/) [[Cite]](ekijcaibibtex.html)

[14] Ezgi Korkmaz. Non-Robust Feature Mapping in Deep Reinforcement Learning. International Conference on Machine Learning (ICML) A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning Workshop, 2021 & Conference on Neural Information Processing Systems (NeurIPS) Metacognition in the Age of AI: Challenges and Opportunities **[Spotlight Presentation]**, 2021. [[Cite]](icmlmapbibtex.html)

[15]  Ezgi Korkmaz. Adversarial Training Blocks Generalization in Neural Policies. International Conference on Learning Representation (ICLR) Robust and Reliable Machine Learning in the Real World Workshop, 2021 & Conference on Neural Information Processing Systems (NeurIPS) Workshop on Distribution Shifts: Connecting Methods and Applications, 2021 & Conference on Neural Information Processing Systems (NeurIPS) Safe and Robust Control of Uncertain Systems Workshop, 2021 & Conference on Neural Information Processing Systems (NeurIPS) I Can't Believe It's Not Better Workshop, 2021. [[Paper ICLR]](iclr.pdf) [[Paper NeurIPS]](KorkmazNeurIPS.pdf) [[Cite]](eknaturalbibtex.html)

[16] Ezgi Korkmaz. Nesterov Momentum Adversarial Perturbations in the Deep Reinforcement Learning Domain. International Conference on Machine Learning (ICML) Inductive Biases, Invariances and Generalization in Reinforcement Learning Workshop, 2020. [[Paper]](https://biases-invariances-generalization.github.io/pdf/big_33.pdf) [[Cite]](ekicmlnesterovbibtex.html)

[17] Ezgi Korkmaz. Adversarially Trained Neural Policies in Fourier Domain. International Conference on Learning Representation (ICLR) Robust and Reliable Machine Learning in the Real World Workshop,2021 & International Conference on Machine Learning (ICML) A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning Workshop, 2021. [[Cite]](ekfourierbibtex.html)

<sup>a</sup> Excluding the study [3].

**Lectures**

Hoeffding's Inequality [[Slides]](HoeffdingsInequalityLecture.pdf)

AI Safety: From Reinforcement Learning to Foundation Models, **AAAI 2025**. [[Link]](https://sites.google.com/view/aisafety-aaai2025) [[AAAI 2025]](AIsafetyaaai.pdf) [[Pdf]](aisafety.pdf) 

