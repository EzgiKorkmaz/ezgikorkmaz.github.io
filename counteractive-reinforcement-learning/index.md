---
layout: default
title: Counteractive Reinforcement Learning
keywords: "Deep Reinforcement Learning, scalable, efficient, sample-efficiency, efficient reinforcement learning, scalable reinforcement learning"
---

<style>
  /* This targets the specific 'Minimal' theme layout */
  .wrapper {
    max-width: 900px !important; /* Makes the page wider */
    margin-left: auto !important;
    margin-right: auto !important;
  }
  header {
    display: none !important; /* This usually removes the sidebar/picture */
  }
  section {
    width: 100% !important;
    float: none !important;
  }
</style>


<div style="text-align: center; margin-bottom: 40px;">


    <h2 style="color: #003366; margin-top: 10px;">
    Efficient and Scalable Reinforcement Learning
  </h2>

  <h3 style="color: #003366; margin-top: 10px;">
    Counteractive RL: Rethinking Core Principles for Efficient and Scalable Deep Reinforcement Learning
  </h3>

  <p style="color: #003366; font-weight: bold; font-size: 1.1em; margin-top: 5px;">
    Conference on Neural Information Processing Systems, NeurIPS 2025 
  </p>

  <p style="color: #003366; font-weight: bold; font-size: 1.1em; margin-top: 5px;">
    ✨ Spotlight Presentation ✨
  </p>


</div>

This paper introduces a fundamental paradigm for reinforcement learning that accelarest learning. Our approach centers on solely reconstituting and conceptually shifting the core
principles of learning and as a result increases the information gained from the environment interactions of the policy in a given MDP without adding computational complexity. Our analysis and method provide a theoretical basis for efficient, effective, scalable and accelerated reinforcement learning.


<div style="text-align: center; margin: 30px 0;">
  
  <a href="https://openreview.net/pdf?id=qaHrpITIvB" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      Paper
    </button>
  </a>

  <a href="https://openreview.net/forum?id=qaHrpITIvB" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      OpenReview
    </button>
  </a>

  <a href="https://ezgikorkmaz.github.io" target="_blank" style="text-decoration: none;">
    <button style="background-color: #003366; color: white; border: none; padding: 10px 20px; border-radius: 25px; font-weight: bold; cursor: pointer; margin: 5px; transition: 0.3s;">
      Author
    </button>
  </a>

</div>


### Citation

<div style="display: flex; justify-content: center; margin-top: 20px;">
  <div style="background-color: #fcfcfc; padding: 20px; border-radius: 8px; border: 1px solid #d1d5da; font-family: 'Courier New', Courier, monospace; line-height: 1.5; width: 100%; max-width: 850px;">
<pre style="margin: 0; white-space: pre-wrap; color: #000000 !important;"><span style="color: #000000; font-weight: bold;">@article</span>{<span style="color: #003366;">korkmaz2025counteractive</span>,
  <span style="color: #000000; font-weight: bold;">title</span>={<span style="color: #003366;">Counteractive RL: Rethinking Core Principles for Efficient and Scalable Deep Reinforcement Learning</span>},
  <span style="color: #000000; font-weight: bold;">author</span>={<span style="color: #003366;">Korkmaz, Ezgi</span>},
  <span style="color: #000000; font-weight: bold;">journal</span>={<span style="color: #003366;">Conference on Neural Information Processing Systems, NeurIPS</span>},
  <span style="color: #000000; font-weight: bold;">year</span>={<span style="color: #003366;">2025</span>},
  <span style="color: #000000; font-weight: bold;">url</span>={<a href="https://openreview.net/pdf?id=qaHrpITIvB" style="color: #003366; text-decoration: underline;">https://openreview.net/pdf?id=qaHrpITIvB</a>}
}</pre>
  </div>
</div>
